import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS
from collections import Counter
import re
from textblob import TextBlob
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer, WordNetLemmatizer
import warnings
warnings.filterwarnings('ignore')

# Descargar recursos de NLTK (solo primera vez)
try:
    nltk.download('punkt')
    nltk.download('stopwords')
    nltk.download('wordnet')
    nltk.download('vader_lexicon')
except:
    print("Los recursos de NLTK ya est√°n descargados")

# Configuraci√≥n
plt.style.use('seaborn-v0_8')
sns.set_palette("husl")
np.random.seed(42)

class TextAnalyzer:
    def __init__(self):
        self.stop_words = set(stopwords.words('english'))
        self.stemmer = PorterStemmer()
        self.lemmatizer = WordNetLemmatizer()
        
        # Palabras espec√≠ficas de basketball para excluir
        self.basketball_stopwords = {
            'nba', 'basketball', 'basket', 'ball', 'game', 'games', 'team', 'teams',
            'player', 'players', 'play', 'playing', 'played', 'season', 'seasons'
        }
        
        self.all_stopwords = self.stop_words.union(self.basketball_stopwords)
    
    def preprocess_text(self, text, use_lemmatization=True, remove_numbers=True):
        """
        Preprocesamiento completo de texto
        """
        # Convertir a min√∫sculas
        text = text.lower()
        
        # Remover n√∫meros si se solicita
        if remove_numbers:
            text = re.sub(r'\d+', '', text)
        
        # Remover puntuaci√≥n y caracteres especiales
        text = re.sub(r'[^\w\s]', ' ', text)
        
        # Tokenizaci√≥n
        tokens = word_tokenize(text)
        
        # Filtrar stopwords y tokens muy cortos
        tokens = [token for token in tokens if token not in self.all_stopwords and len(token) > 2]
        
        # Lematizaci√≥n o stemming
        if use_lemmatization:
            tokens = [self.lemmatizer.lemmatize(token) for token in tokens]
        else:
            tokens = [self.stemmer.stem(token) for token in tokens]
        
        return ' '.join(tokens)
    
    def analyze_sentiment(self, text):
        """
        An√°lisis de sentimiento usando TextBlob
        """
        blob = TextBlob(text)
        return blob.sentiment.polarity, blob.sentiment.subjectivity
    
    def get_top_ngrams(self, text, n=1, top_k=20):
        """
        Obtener n-gramas m√°s frecuentes
        """
        tokens = text.split()
        if n == 1:
            ngrams = tokens
        else:
            ngrams = [' '.join(tokens[i:i+n]) for i in range(len(tokens)-n+1)]
        
        counter = Counter(ngrams)
        return counter.most_common(top_k)
    
    def create_advanced_wordcloud(self, text, title, filename, 
                                width=800, height=400, 
                                background_color='white',
                                colormap='viridis'):
        """
        Crear wordcloud avanzado con m√∫ltiples opciones
        """
        # Crear m√°scara opcional (podr√≠as a√±adir una forma espec√≠fica)
        # Por ahora usamos None para forma rectangular
        
        wordcloud = WordCloud(
            width=width,
            height=height,
            background_color=background_color,
            colormap=colormap,
            stopwords=self.all_stopwords,
            max_words=200,
            min_font_size=8,
            max_font_size=100,
            random_state=42,
            relative_scaling=0.5,
            collocations=True
        ).generate(text)
        
        # Crear visualizaci√≥n
        plt.figure(figsize=(12, 8))
        plt.imshow(wordcloud, interpolation='bilinear')
        plt.axis('off')
        plt.title(f'Word Cloud: {title}', fontsize=16, pad=20)
        plt.tight_layout()
        plt.savefig(f'Practica9/{filename}.png', dpi=300, bbox_inches='tight')
        plt.show()
        
        return wordcloud

# AN√ÅLISIS 1: NOMBRES DE JUGADORES
print("=" * 80)
print("AN√ÅLISIS 1: NOMBRES DE JUGADORES NBA")
print("=" * 80)

# Cargar datos
df = pd.read_csv("edited-salaries.csv")
print(f"Dataset shape: {df.shape}")
print(f"Total de jugadores √∫nicos: {df['name'].nunique()}")

# Inicializar analizador
analyzer = TextAnalyzer()

# Preprocesar nombres de jugadores
all_names = ' '.join(df['name'].astype(str))
preprocessed_names = analyzer.preprocess_text(all_names, use_lemmatization=False)

print(f"\nESTAD√çSTICAS DE NOMBRES:")
print(f"Total de caracteres: {len(all_names):,}")
print(f"Total de palabras (preprocesadas): {len(preprocessed_names.split()):,}")

# An√°lisis de apellidos m√°s comunes
names_list = df['name'].str.split().explode()
surnames = names_list[names_list.str.len() > 2]  # Filtrar nombres muy cortos
surname_counts = surnames.value_counts().head(20)

print(f"\nAPELLIDOS M√ÅS COMUNES EN LA NBA:")
for surname, count in surname_counts.head(10).items():
    print(f"  {surname}: {count} jugadores")

# Wordcloud de nombres
print("\nGENERANDO WORDCLOUD DE NOMBRES...")
name_wordcloud = analyzer.create_advanced_wordcloud(
    preprocessed_names, 
    "Nombres de Jugadores NBA (2000-2009)",
    "wordcloud_names",
    colormap='plasma'
)

# AN√ÅLISIS 2: POSICIONES Y EQUIPOS
print("\n" + "=" * 80)
print("AN√ÅLISIS 2: POSICIONES Y EQUIPOS")
print("=" * 80)

# Combinar posiciones y equipos
positions_text = ' '.join(df['position'].astype(str) * 3)  # Peso extra para posiciones
teams_text = ' '.join(df['team'].astype(str))

basketball_terms = positions_text + " " + teams_text
preprocessed_basketball = analyzer.preprocess_text(basketball_terms)

print("T√âRMINOS DE BASKETBALL M√ÅS COMUNES:")
basketball_ngrams = analyzer.get_top_ngrams(preprocessed_basketball, n=1, top_k=15)
for term, count in basketball_ngrams:
    print(f"  {term}: {count}")

# Wordcloud de t√©rminos de basketball
print("\nGENERANDO WORDCLOUD DE T√âRMINOS NBA...")
basketball_wordcloud = analyzer.create_advanced_wordcloud(
    preprocessed_basketball,
    "T√©rminos de Basketball (Posiciones y Equipos)",
    "wordcloud_basketball_terms",
    colormap='cool'
)

# AN√ÅLISIS 3: COMBINACI√ìN DE TEXTO COMPLETO
print("\n" + "=" * 80)
print("AN√ÅLISIS 3: TEXTO COMPLETO DEL DATASET")
print("=" * 80)

# Combinar todas las columnas de texto
all_text_data = ""
text_columns = ['name', 'position', 'team']

for col in text_columns:
    if col in df.columns:
        column_text = ' '.join(df[col].astype(str))
        all_text_data += column_text + " "

preprocessed_all = analyzer.preprocess_text(all_text_data)

print(f"ESTAD√çSTICAS DEL TEXTO COMPLETO:")
print(f"Palabras √∫nicas: {len(set(preprocessed_all.split())):,}")
print(f"Palabra m√°s larga: {max(preprocessed_all.split(), key=len)}")

# An√°lisis de n-gramas
print(f"\nüî§ N-GRAMAS M√ÅS FRECUENTES:")

for n in [1, 2, 3]:
    ngrams = analyzer.get_top_ngrams(preprocessed_all, n=n, top_k=8)
    print(f"\n{n}-gramas:")
    for ngram, count in ngrams:
        print(f"  '{ngram}': {count}")

# Wordcloud completo
print("\nGENERANDO WORDCLOUD COMPLETO...")
complete_wordcloud = analyzer.create_advanced_wordcloud(
    preprocessed_all,
    "An√°lisis Textual Completo - NBA Dataset",
    "wordcloud_complete",
    colormap='viridis'
)

# VISUALIZACIONES ADICIONALES
print("\n" + "=" * 80)
print("VISUALIZACIONES ADICIONALES")
print("=" * 80)

# 1. GR√ÅFICO DE BARRAS - Apellidos m√°s comunes
plt.figure(figsize=(12, 8))
top_surnames = surname_counts.head(15)
plt.barh(top_surnames.index, top_surnames.values, color=sns.color_palette("husl", len(top_surnames)))
plt.xlabel('Frecuencia')
plt.ylabel('Apellido')
plt.title('Apellidos M√°s Comunes en la NBA (2000-2009)')
plt.gca().invert_yaxis()
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('Practica9/top_surnames.png', dpi=300, bbox_inches='tight')
plt.show()

# 2. GR√ÅFICO DE BARRAS - Posiciones m√°s comunes
plt.figure(figsize=(10, 6))
position_counts = df['position'].value_counts()
plt.bar(position_counts.index, position_counts.values, color='lightcoral')
plt.xlabel('Posici√≥n')
plt.ylabel('N√∫mero de Jugadores')
plt.title('Distribuci√≥n de Posiciones en la NBA')
plt.xticks(rotation=45)
plt.grid(True, alpha=0.3)
plt.tight_layout()
plt.savefig('Practica9/position_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# 3. GR√ÅFICO DE TORTA - Distribuci√≥n de equipos (top 10)
plt.figure(figsize=(10, 8))
team_counts = df['team'].value_counts().head(10)
plt.pie(team_counts.values, labels=team_counts.index, autopct='%1.1f%%', startangle=90)
plt.title('Top 10 Equipos con M√°s Jugadores (2000-2009)')
plt.tight_layout()
plt.savefig('Practica9/team_distribution.png', dpi=300, bbox_inches='tight')
plt.show()

# 4. AN√ÅLISIS DE LONGITUD DE NOMBRES
plt.figure(figsize=(12, 6))

# Longitud de nombres completos
name_lengths = df['name'].str.split().str.join('').str.len()

plt.subplot(1, 2, 1)
plt.hist(name_lengths, bins=20, alpha=0.7, color='skyblue', edgecolor='black')
plt.xlabel('Longitud del Nombre (caracteres)')
plt.ylabel('Frecuencia')
plt.title('Distribuci√≥n de Longitud de Nombres')
plt.grid(True, alpha=0.3)

# N√∫mero de palabras por nombre
word_counts = df['name'].str.split().str.len()

plt.subplot(1, 2, 2)
word_count_dist = word_counts.value_counts().sort_index()
plt.bar(word_count_dist.index, word_count_dist.values, color='lightgreen', alpha=0.7)
plt.xlabel('N√∫mero de Palabras en el Nombre')
plt.ylabel('Frecuencia')
plt.title('N√∫mero de Palabras por Nombre')
plt.grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('Practica9/name_analysis.png', dpi=300, bbox_inches='tight')
plt.show()

# AN√ÅLISIS DE PATRONES ESPEC√çFICOS
print("\n" + "=" * 80)
print("AN√ÅLISIS DE PATRONES ESPEC√çFICOS")
print("=" * 80)

# Patrones en nombres
print("üîç PATRONES EN NOMBRES DE JUGADORES:")

# Nombres que contienen "Jr", "Sr", "III", etc.
special_patterns = ['jr', 'sr', 'ii', 'iii', 'iv', 'v']
for pattern in special_patterns:
    count = df['name'].str.lower().str.contains(pattern, na=False).sum()
    if count > 0:
        print(f"  Nombres con '{pattern.upper()}': {count}")

# Nombres con ap√≥strofe
apostrophe_count = df['name'].str.contains("'", na=False).sum()
print(f"  Nombres con ap√≥strofe: {apostrophe_count}")

# Jugadores con el mismo apellido (posibles familiares)
surname_duplicates = surnames.value_counts()
common_surnames = surname_duplicates[surname_duplicates > 1]
print(f"\nüë• Apellidos compartidos por m√∫ltiples jugadores: {len(common_surnames)}")

# Mostrar algunos ejemplos
print("\nEjemplos de apellidos compartidos:")
for surname, count in common_surnames.head(5).items():
    players = df[df['name'].str.contains(surname, case=False, na=False)]['name'].tolist()
    print(f"  {surname} ({count}): {', '.join(players[:3])}")

# COMPARACI√ìN ENTRE WORDCLOUDS
print("\n" + "=" * 80)
print("COMPARACI√ìN ENTRE WORDCLOUDS")
print("=" * 80)

# Crear figura comparativa
fig, axes = plt.subplots(2, 2, figsize=(16, 12))

# Wordcloud 1: Nombres
axes[0, 0].imshow(name_wordcloud, interpolation='bilinear')
axes[0, 0].set_title('Nombres de Jugadores', fontsize=14)
axes[0, 0].axis('off')

# Wordcloud 2: T√©rminos basketball
axes[0, 1].imshow(basketball_wordcloud, interpolation='bilinear')
axes[0, 1].set_title('T√©rminos de Basketball', fontsize=14)
axes[0, 1].axis('off')

# Wordcloud 3: Completo
axes[1, 0].imshow(complete_wordcloud, interpolation='bilinear')
axes[1, 0].set_title('An√°lisis Completo', fontsize=14)
axes[1, 0].axis('off')

# Gr√°fico de frecuencias
top_words = analyzer.get_top_ngrams(preprocessed_all, n=1, top_k=10)
words, counts = zip(*top_words)
axes[1, 1].barh(words, counts, color='lightblue')
axes[1, 1].set_title('Top 10 Palabras M√°s Frecuentes', fontsize=14)
axes[1, 1].set_xlabel('Frecuencia')
axes[1, 1].grid(True, alpha=0.3)
axes[1, 1].invert_yaxis()

plt.tight_layout()
plt.savefig('Practica9/wordclouds_comparison.png', dpi=300, bbox_inches='tight')
plt.show()

# EXPORTACI√ìN DE RESULTADOS
print("\n" + "=" * 80)
print("EXPORTACI√ìN DE RESULTADOS")
print("=" * 80)

# Crear resumen de an√°lisis
analysis_summary = {
    'total_jugadores': len(df),
    'jugadores_unicos': df['name'].nunique(),
    'total_palabras_preprocesadas': len(preprocessed_all.split()),
    'palabras_unicas': len(set(preprocessed_all.split())),
    'apellidos_comunes': len(common_surnames),
    'posiciones_unicas': df['position'].nunique(),
    'equipos_unicos': df['team'].nunique()
}

summary_df = pd.DataFrame([analysis_summary])
summary_df.to_csv('Practica9/analysis_summary.csv', index=False)

# Exportar frecuencias de palabras
word_frequencies = analyzer.get_top_ngrams(preprocessed_all, n=1, top_k=50)
freq_df = pd.DataFrame(word_frequencies, columns=['word', 'frequency'])
freq_df.to_csv('Practica9/word_frequencies.csv', index=False)

print("üìÅ ARCHIVOS EXPORTADOS:")
print("  Practica9/wordcloud_names.png")
print("  Practica9/wordcloud_basketball_terms.png")
print("  Practica9/wordcloud_complete.png")
print("  Practica9/wordclouds_comparison.png")
print("  Practica9/top_surnames.png")
print("  Practica9/position_distribution.png")
print("  Practica9/team_distribution.png")
print("  Practica9/name_analysis.png")
print("  Practica9/analysis_summary.csv")
print("  Practica9/word_frequencies.csv")

print(f"\nHALLAZGOS PRINCIPALES:")
print(f"  ‚Ä¢ {analysis_summary['jugadores_unicos']} jugadores √∫nicos analizados")
print(f"  ‚Ä¢ {analysis_summary['palabras_unicas']} palabras √∫nicas identificadas")
print(f"  ‚Ä¢ {analysis_summary['apellidos_comunes']} apellidos compartidos entre jugadores")
print(f"  ‚Ä¢ Patrones identificados en nombres y terminolog√≠a basketball")

print("\n" + "=" * 80)
print("¬°AN√ÅLISIS TEXTUAL Y WORD CLOUDS COMPLETADO!")
print("=" * 80)